{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae74d3c1",
   "metadata": {},
   "source": [
    "Q1. R-squared in Linear Regression\n",
    "R-squared (R²) is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variables in a linear regression model. It ranges from 0 to 1, where 1 indicates a perfect fit.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "Calculate the total sum of squares (TSS): Σ(yᵢ - ȳ)²\n",
    "Calculate the residual sum of squares (RSS): Σ(yᵢ - ȳ_pred)²\n",
    "R-squared = 1 - (RSS / TSS)\n",
    "Explanation:\n",
    "\n",
    "R-squared measures the goodness of fit of a regression model. A higher R-squared indicates that the model explains more variance in the dependent variable.\n",
    "It does not indicate whether the regression model is biased or unbiased, only the quality of the fit.\n",
    "R-squared can be misleading when used with complex models or with irrelevant predictors.\n",
    "Q2. Adjusted R-squared\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes excessive use of variables that do not significantly improve the model's fit.\n",
    "\n",
    "Calculation:\n",
    "Adjusted R-squared = 1 - (1 - R²) * ((n - 1) / (n - p - 1))\n",
    "where n = sample size, p = number of predictors\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Adjusted R-squared penalizes models with many predictors, thus providing a more accurate assessment of model fit.\n",
    "It tends to decrease when irrelevant predictors are added to the model.\n",
    "Q3. Appropriate Use of Adjusted R-squared\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps in identifying whether additional predictors improve the model fit significantly or not.\n",
    "\n",
    "Q4. RMSE, MSE, and MAE\n",
    "RMSE (Root Mean Squared Error) represents the square root of the average of squared differences between predicted and actual values.\n",
    "MSE (Mean Squared Error) is the average of squared differences between predicted and actual values.\n",
    "MAE (Mean Absolute Error) is the average of absolute differences between predicted and actual values.\n",
    "Calculation:\n",
    "\n",
    "RMSE = √(Σ(yᵢ - ŷᵢ)² / n)\n",
    "MSE = Σ(yᵢ - ŷᵢ)² / n\n",
    "MAE = Σ|yᵢ - ŷᵢ| / n\n",
    "Explanation:\n",
    "\n",
    "RMSE, MSE, and MAE are used to evaluate the accuracy of regression models by measuring the error between predicted and actual values.\n",
    "Lower values of RMSE, MSE, and MAE indicate better model performance.\n",
    "Q5. Advantages and Disadvantages of Evaluation Metrics\n",
    "Advantages:\n",
    "\n",
    "RMSE, MSE, and MAE are easy to interpret and understand.\n",
    "They provide quantitative measures of model performance.\n",
    "They penalize large errors more heavily, making them sensitive to outliers.\n",
    "Disadvantages:\n",
    "\n",
    "RMSE and MSE give more weight to large errors, which may not always be desirable.\n",
    "MAE does not differentiate between the magnitude of overestimation and underestimation.\n",
    "Q6. Lasso Regularization\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to penalize the absolute size of coefficients, forcing some of them to be exactly zero. It differs from Ridge regularization in that it can lead to sparse models with fewer predictors.\n",
    "\n",
    "Q7. Preventing Overfitting with Regularized Linear Models\n",
    "Regularized linear models penalize the complexity of the model, preventing it from fitting the noise in the training data too closely. For example, Ridge regression adds a penalty term to the loss function, forcing the model to generalize better to unseen data.\n",
    "\n",
    "Q8. Limitations of Regularized Linear Models\n",
    "Regularized linear models assume a linear relationship between predictors and the target variable, which may not always be the case. They may also introduce bias if the regularization parameter is not chosen appropriately.\n",
    "\n",
    "Q9. Choosing between Models\n",
    "Model B with an MAE of 8 would be chosen as the better performer, as it has a lower error compared to Model A. However, the choice of metric may be limited as different metrics prioritize different aspects of model performance.\n",
    "\n",
    "Q10. Choosing between Regularization Methods\n",
    "The choice between Ridge and Lasso regularization depends on the specific characteristics of the dataset. In this case, the better performer would be Model A using Ridge regularization, as it has a lower regularization parameter. However, there may be trade-offs between sparsity and predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e49dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1127d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ade4f05",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
